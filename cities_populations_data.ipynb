{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping for cities information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#!pip install --upgrade beautifulsoup4\n",
    "#!pip install requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting data for one city (Berlin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find url and store it in a variable\n",
    "url = \"https://en.wikipedia.org/wiki/Berlin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download html with a get request\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.8'}\n",
    "response = requests.get(url, headers = headers)\n",
    "response.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html (create the 'soup')\n",
    "wiki_soup_fr = BeautifulSoup(response.content, \"html.parser\")\n",
    "# check that the html code looks like it should\n",
    "wiki_soup_fr.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = wiki_soup_fr.select(\"span.mw-page-title-main\")[0].get_text()\n",
    "country_name = wiki_soup_fr.select(\"table.infobox td.infobox-data a\")[0].get_text()\n",
    "latitude = wiki_soup_fr.select(\"span.latitude\")[0].get_text()\n",
    "longitude = wiki_soup_fr.select(\"span.longitude\")[0].get_text()\n",
    "# population_1 = wiki_soup_fr.select(\"td.infobox-data\")[10].get_text()\n",
    "population_2 = wiki_soup_fr.select_one('th.infobox-header:-soup-contains(\"Population\")').parent.find_next_sibling().find(text=re.compile(r'\\d+')) # better method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame\n",
    "scooter_cities_df= pd.DataFrame(\n",
    "    {\"city\": [city_name],\n",
    "     \"country\": [country_name],\n",
    "     \"latitude\": [latitude],\n",
    "     \"longitude\": [longitude],\n",
    "     \"population\": [population_2]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Looping to add other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cities = [\"Frankfurt\", \"Hamburg\", \"Cologne\", \"Munich\"] # input\n",
    "\n",
    "for city in list_of_cities:\n",
    "\n",
    "    url = \"https://en.wikipedia.org/wiki/\" + city\n",
    "    #download html with a get request\n",
    "    headers = {'Accept-Language': 'en-US,en;q=0.8'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    if response.status_code != 200: break  # 200 status code means OK!\n",
    "\n",
    "    # parse html (create the 'soup')\n",
    "    wiki_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    #extract name, country, latitude, longitude, population\n",
    "    city_name = wiki_soup.select(\"span.mw-page-title-main\")[0].getText()\n",
    "    country_name = wiki_soup.select(\"table.infobox td.infobox-data\")[0].getText()\n",
    "    latitude = wiki_soup.select(\"span.latitude\")[0].getText()\n",
    "    longitude = wiki_soup.select(\"span.longitude\")[0].getText()\n",
    "\n",
    "    if wiki_soup.select_one('th.infobox-header:-soup-contains(\"Population\")'):\n",
    "        population = wiki_soup.select_one('th.infobox-header:-soup-contains(\"Population\")').parent.find_next_sibling().find(text=re.compile(r'\\d+'))\n",
    "\n",
    "    #append information to the cities_df\n",
    "    city_df = pd.DataFrame(\n",
    "        {\"city\": [city_name],\n",
    "         \"country\": [country_name],\n",
    "         \"latitude\": [latitude],\n",
    "         \"longitude\": [longitude],\n",
    "         \"population\": [population]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #scooter_cities_df = scooter_cities_df.append(city_df, ignore_index=True)\n",
    "    scooter_cities_df = pd.concat([scooter_cities_df, city_df], ignore_index = True)\n",
    "\n",
    "    # fixing latitude\n",
    "    scooter_cities_df['latitude'] = scooter_cities_df['latitude'].str.split('″').str[0].str.replace('°', '.', regex=False).str.replace('′', '', regex=False)\n",
    "    # fixing longitude\n",
    "    scooter_cities_df['longitude'] = scooter_cities_df['longitude'].str.split('″').str[0].str.replace('°', '.', regex=False).str.replace('′', '', regex=False)\n",
    "\n",
    "    # fixing population\n",
    "    scooter_cities_df[\"population\"] = scooter_cities_df[\"population\"].str.replace(',', '', regex=False)\n",
    "\n",
    "scooter_cities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pushing data to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=\"gans\" \n",
    "host=\"wbs-project3-db.cunuvsto5hvy.us-east-1.rds.amazonaws.com\"\n",
    "user=\"admin\"\n",
    "password = \"***\"\n",
    "port=3306\n",
    "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_sql_df = scooter_cities_df.loc[:, ['city', 'country']]\n",
    "cities_sql_df['country'] = \"DE\"\n",
    "cities_sql_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_sql_df.to_sql('cities', \n",
    "              if_exists='append', \n",
    "              con=con, \n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_sql_df = scooter_cities_df.loc[:, ['population']]\n",
    "populations_sql_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_sql_df.to_sql('populations', \n",
    "              if_exists='append', \n",
    "              con=con, \n",
    "              index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience_Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
